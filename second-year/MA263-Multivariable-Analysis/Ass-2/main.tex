% vim: set foldmethod=marker foldlevel=0:

\documentclass[a4paper]{article}
\usepackage[UKenglish]{babel}

% \usepackage[hidelinks]{hyperref}

\usepackage{preamble}

% \usepackage{graphicx}
% \graphicspath{ {./imgs/} }

% \renewcommand{\thesubsection}{Q\arabic{section}~(\roman{subsection})}

\fancyhead[L]{MA263 Assignment 2}
\title{MA263 Multivariable Analysis, Assignment 2}
\colorlet{questionbodycolor}{cyan!50}

\begin{document}

\maketitle

\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

% {{{ Q1
\question{1}

\begin{questionbody}
Compute the second order Taylor polynomial of $f(x, y) = \arctan(x + y)$ at $(0, 0)$.
\end{questionbody}

We want
\begin{align*}
T_{\ul 0}^2 f(\ul x) &= f(\ul 0) + \nabla f(\ul 0) \cdot (\ul x - \ul 0) + \f12 {(\ul x - \ul 0)}^T \nabla^2 f(\ul 0) (\ul x - \ul 0) \\
&= f(\ul 0) + \nabla f(\ul 0) \cdot \ul x + \f12 {\ul x}^T \nabla^2 f(\ul 0) \ul x,
\end{align*}
where $\ul x = (x, y)$ and $\ul 0 = (0, 0)$.

We first compute the gradient and Hessian.
\begin{align*}
\nabla f(x, y) &= \begin{pmatrix}
\partial_x f \\
\partial_y f
\end{pmatrix} \\
&= \begin{pmatrix}
\f1{{(x + y)}^2 + 1} \\
\f1{{(x + y)}^2 + 1}
\end{pmatrix} \\[1.5ex]
%
\nabla^2 f(x, y) &= \begin{pmatrix}
\partial_x^2 f & \partial_{xy} f \\
\partial_{yx} f & \partial_y^2 f
\end{pmatrix} \\
&= \begin{pmatrix}
- \f{2 (x + y)}{{( {(x + y)}^2 + 1 )}^2} & - \f{2 (x + y)}{{( {(x + y)}^2 + 1 )}^2} \\
- \f{2 (x + y)}{{( {(x + y)}^2 + 1 )}^2} & - \f{2 (x + y)}{{( {(x + y)}^2 + 1 )}^2}
\end{pmatrix}
\end{align*}

Then we evaluate these at $\ul 0$.
\begin{align*}
\nabla f(0, 0) &= \begin{pmatrix}
\f1{0^2 + 1} \\
\f1{0^2 + 1}
\end{pmatrix} \\
&= \begin{pmatrix}
1 \\
1
\end{pmatrix} \\[1.5ex]
%
\nabla^2 f(x, y) &= \begin{pmatrix}
- \f{2 (0)}{{( 0^2 + 1 )}^2} & - \f{2 (0)}{{( 0^2 + 1 )}^2} \\
- \f{2 (0)}{{( 0^2 + 1 )}^2} & - \f{2 (0)}{{( 0^2 + 1 )}^2}
\end{pmatrix} \\
&= \begin{pmatrix}
0 & 0 \\
0 & 0
\end{pmatrix}
\end{align*}

And so,
\begin{align*}
T_{\ul 0}^2 f(\ul x) &= f(\ul 0) + \nabla f(\ul 0) \cdot \ul x + \f12 {\ul x}^T \nabla^2 f(\ul 0) \ul x \\
&= \arctan(0) + \begin{pmatrix} 1 \\ 1 \end{pmatrix} \cdot \begin{pmatrix} x \\ y \end{pmatrix} + \f12 \begin{pmatrix} x & y \end{pmatrix} \begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} \\
&= 0 + x + y + 0 \\
&= x + y.
\end{align*}

% }}}

% {{{ Q2
\newquestion{2}

\begin{questionbody}
Prove the following corollary.

Let $U \subset \R^2$ and $f \in C^2(U; \R)$ have a critical point at $a \in U$. If the determinant of the Hessian is positive then $f$ is a strict local extremum, and if it is negative then $f$ is a saddle point.

In the former case, it is a maximum if and only if the diagonal entries of $\nabla^2 f(a)$ are negative and a minimum if and only if they are positive.
\end{questionbody}

% Do some linear algebra and appeal to the second derivative test.
% Recall the trace, which is the sum of the diagonal entries, and also the sum of the eigenvalues.

The Hessian of $f$ is \[
\nabla^2 f = \begin{pmatrix}
\partial_x^2 f & \partial_x \partial_y f \\
\partial_y \partial_x f & \partial_y^2 f
\end{pmatrix}.
\]
Therefore \[
\det \nabla^2 f = \l(\partial_x^2 f\r) \l(\partial_y^2 f\r) - (\partial_x \partial_y f) (\partial_y \partial_x f),
\] which is \[
\l(\partial_x^2 f\r) \l(\partial_y^2 f\r) - {(\partial_x \partial_y f)}^2
\] by Clairaut's~theorem.

We know that the determinant of a matrix is the product of its eigenvalues. The second derivative test tells us that if both eigenvalues are positive or both are negative, then $f$ has a strict local extremum at $a$. This is equivalent to the determinant being positive.

The second derivative test also tells us that if one eigenvalue is positive and one is negative then $f$ has a saddle point at $a$. This is equivalent to the derivative being negative. If either eigenvalue is zero, we can't conclude anything.

Considering only the former case, we know that $\det \nabla^2 f > 0$ and so either both eigenvalues are positive or both are negative. We know that the trace of a matrix (the sum of its diagonal entries) is the sum of its eigenvalues.

The second derivative test tells us that if both eigenvalues are negative, and hence the trace is negative, then $f$ has a local maximum at $a$. Likewise, if both eigenvalues are positive, and hence the trace is positive, then $f$ has a local minimum at $a$.

Since we're only consider the case where we already know $\det \nabla^2 f > 0$, we that either both eigenvalues are positive or both are negative, so the logic also works conversely.

\hfill $\square$

% }}}

% {{{ Q3
\newquestion{3}

\begin{questionbody}
Using the optimisation methods of calculus, find the global maximum and minimum of \[
f(x, y) = 2x^2 + 6xy - 26x + 5y^2 - 42y + 93
\] over ${[0, 4]}^2$.
\end{questionbody}

The gradient is \[
\nabla f = \begin{pmatrix}
4x + 6y - 26 \\
6x + 10y - 42
\end{pmatrix},
\] and the Hessian is \[
\nabla^2 f = \begin{pmatrix}
4 & 6 \\
6 & 10
\end{pmatrix}.
\]

The eigenvalues are both positive, so any critical point of $f$ will be a minimum.

To find critical points, we set $\nabla f = 0$ and get the simultaneous equations
\begin{align*}
4x + 6y &= 26 \\
6x + 10y &= 42
\end{align*}

We invert the matrix to find $x$ and $y$:
\begin{align*}
\det \begin{pmatrix}
4 & 6 \\
6 & 10
\end{pmatrix} &= 4 \\
\begin{pmatrix}
4 & 6 \\
6 & 10
\end{pmatrix}^{-1}
&= \f14 \begin{pmatrix}
10 & -6 \\
-6 & 4
\end{pmatrix} \\
&= \begin{pmatrix}
\f52 & -\f32 \\
-\f32 & 1
\end{pmatrix} \\
\begin{pmatrix}
x \\
y
\end{pmatrix}
&= \begin{pmatrix}
\f52 & -\f32 \\
-\f32 & 1
\end{pmatrix} \begin{pmatrix}
26 \\
42
\end{pmatrix} \\
&= \begin{pmatrix}
65 - 63 \\
-39 + 42
\end{pmatrix} \\
&= \begin{pmatrix}
2 \\
3
\end{pmatrix}
\end{align*}
And $f(2, 3) = 4$, so $(2, 3, 4)$ is the global minimum of $f$ on all $\R^2$.

The global maximum of $f$ on ${[0, 4]}^2$ must be somewhere on the boundary. To find it, we will evaluate the gradient at each corner and do gradient ascent.

\begin{align*}
\nabla f(0, 0) &= \begin{pmatrix}
-26 \\
-42
\end{pmatrix} \\
\nabla f(0, 4) &= \begin{pmatrix}
24 - 26 \\
40 - 42
\end{pmatrix} \\
&= \begin{pmatrix}
-2 \\
-2
\end{pmatrix} \\
\nabla f(4, 0) &= \begin{pmatrix}
16 - 26 \\
24 - 42
\end{pmatrix} \\
&= \begin{pmatrix}
-10 \\
-18
\end{pmatrix} \\
\nabla f(4, 4) &= \begin{pmatrix}
16 + 24 - 26 \\
24 + 40 - 42
\end{pmatrix} \\
&= \begin{pmatrix}
14 \\
22
\end{pmatrix}
\end{align*}
Since all the gradients point outside the square ${[0, 4]}^2$, the global maximum must be at one of the corners, so we just need to evaluate $f$ at each corner:
\begin{align*}
f(0, 0) &= 93 \\
f(0, 4) &= 5 \\
f(4, 0) &= 21 \\
f(4, 4) &= 29
\end{align*}
Therefore $(0, 0, 93)$ is the global maximum of $f$ on ${[0, 4]}^2$.

% }}}

% {{{ Q4
\newquestion{4}

\begin{questionbody}
Consider $f : \R^2 \to \R^2$ defined by \[
f(x, y) = (x^2 - y^2, y - x).
\] Find all points near which $f$ is locally invertible. Explain why your list is complete.
\end{questionbody}

The Fr\'echet derivative is
\begin{align*}
D f(x, y) &= \partial f(x, y) \\
&= \begin{pmatrix}
\partial_x (x^2 - y^2) & \partial_y (x^2 - y^2) \\
\partial_x (y - x) & \partial_y (y - x)
\end{pmatrix} \\
&= \begin{pmatrix}
2x & -2y \\
-1 & 1
\end{pmatrix}
\end{align*}

To apply the Inverse Function Theorem at a point $(x, y)$, we require $D f(x, y)$ to be invertible.
\[
\det \begin{pmatrix}
2x & -2y \\
-1 & 1
\end{pmatrix} = 2x - 2y,
\] so we just require $x \ne y$.

We also require $f$ to be $C^k(U, \R^2)$ for some open set $U \subset \R^2$, but $f$ is $C^\infty$ everywhere, so this isn't a problem.

Therefore $f$ is invertible in a small open set $U$ around any point $(x, y)$ with $x \ne y$.

% }}}

\end{document}
